{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task - Land Classification\n",
    "\n",
    "## Column Description:\n",
    "- Numeric columns *X1 to X6 and I1 to I6* define characteristics about the land piece\n",
    "- *target *is the output categorical column which needs to be found for the test dataset\n",
    "    * 1 = Green Land\n",
    "    * 2 = Water\n",
    "    * 3 = Barren Land\n",
    "    * 4 = Built-up \n",
    "\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Explanation of My Solution to the given task :\n",
    "\n",
    "I have tried all the possibilites and check the model performance by training the model on the preprocessed dataset.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------\n",
    "\n",
    "## EDA (Exploratory Data Analysis)\n",
    "\n",
    "1. Checking whether there are any null values.\n",
    "2. Checking the description of data.\n",
    "3. Correlated Features :\n",
    "     1. PairPlot\n",
    "     2. HeatMap \n",
    "4. Checking the distribution Percentage of the target variable.\n",
    "5. Checking of Outliers using Box Plot.\n",
    "\n",
    "\n",
    "## Procedure that I have done with Observations: \n",
    "\n",
    "#### 1. Checking the null counts\n",
    "- No Null Counts are present in the dataset\n",
    "\n",
    "#### 2. Checking the description of the dataset\n",
    "- mean value, std , minimum value and maximum value is small in all I columns and very much larger in X Columns.\n",
    " - To handle this standardization of features are done.\n",
    " \n",
    "#### 3. Checking What are the Correlated Features in the Dataset ?\n",
    "- Graphs made to visualize the correlation between features.\n",
    " - PairPlot\n",
    " - Correlation HeatMap\n",
    "- I have Grouped together those features whose pearson correlation value >0.90 or <-0.90 . Threshold value = 0.90 is taken here .\n",
    " - [ X1 , X2 , X3 ] are correlated heavily \n",
    " - [ X5 , X6 ] are correlated heavily\n",
    " - [ I1 , I3 , I4 ] are correlated heavily\n",
    " \n",
    "#### 4. I have made a new dataset after taking only one feature among these correlated features.\n",
    "- I have used Mutual Information to select the best feature which is more dependant on target variable.\n",
    " - Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n",
    "- I1 is best among I1,I3,I4\n",
    "- X3 is best among X1,X2,X3\n",
    "- X6 seems to be the best among X5,X6\n",
    " \n",
    "#### 5. Removing correlated features\n",
    "- After Removing features we get : X3 , X4 , X6 , I1 , I2 , I5 , I6 \n",
    "\n",
    "#### 6. I have seen the distribution of target variables.\n",
    "- Observations (Imbalanced distribution of target variable):\n",
    " - Percentage of 1 : 45.4545454545\n",
    " - Percentage of 2 : 27.2727272727\n",
    " - Percentage of 3 : 18.1818181818\n",
    " - Percentage of 4 : 9.09090909091\n",
    "- To Handle this problem I have used oversampling technique - SMOTE\n",
    "\n",
    "#### 7. Now I have trained the models on the preprocessed data in which I have removed the correlated features.\n",
    "- Models that are trained Along with the Observations : \n",
    " - Logistic Regression : \n",
    "   - Accuracy : Training Accuracy ( 89 % ) , Testing Accuracy (89 % ) \n",
    "    - Precision (Test Set) : 0.90 \n",
    "    - Recall (Test Set):  0.89\n",
    "    - F1 Score ( weighted ) (Test Set): 0.89\n",
    "\n",
    " - SVM : \n",
    "   - Accuracy : Training Accuracy ( 91 % ) , Testing Accuracy (91 % ) \n",
    "   - Precision (Test Set): 0.92\n",
    "   - Recall (Test Set): 0.91\n",
    "   - F1 Score ( weighted ) (Test Set): 0.92\n",
    "\n",
    " - XGBoost : \n",
    "   - Accuracy : Training Accuracy (96.22 % ) , Testing Accuracy (96.07 % ) \n",
    "   - F1 Score ( weighted )(Test Set) :  0.9612\n",
    "   \n",
    "   \n",
    "### Note Very Important Thing :\n",
    "\n",
    "- We have just removed the correlated features from the dataset . But we should not just remove the features directly from the dataset as there will be loss of information which they will provide the model to classify.\n",
    "\n",
    "- Instead just removing the features from the dataset. We should train the model with L2 penalty keeping all the correalted features in the dataset. This is the best practises to train the model with correlated features.   \n",
    "\n",
    "\n",
    "#### 8.  I check the model performance by training it to preprocessed data without removing the correlated features.\n",
    "- Reason : \n",
    " - Check Above Note.\n",
    " - By Checking out the performance of the model ,I found out that dataset with correlated features gives the model more information to classify than dataset having removed correlated features.\n",
    " \n",
    "- Models that are trained Along with the Observations :\n",
    " - Logistic Regression\n",
    "   - Accuracy : Training Accuracy ( 93 % ) , Testing Accuracy (94 % ) \n",
    "   - Precision (Test Set) : 0.94\n",
    "   - Recall (Test Set): 0.94\n",
    "   - F1 Score ( weighted ) (Test Set) : 0.94\n",
    "\n",
    " - SVM\n",
    "   - Accuracy : Training Accuracy (94 % ) , Testing Accuracy (94 % ) \n",
    "   - Precision (Test Set): 0.95\n",
    "   - Recall (Test Set): 0.94\n",
    "   - F1 Score ( weighted ) (Test Set):  0.94\n",
    "\n",
    " - XGBoost\n",
    "   - Accuracy : Training Accuracy ( 96.92% ) , Testing Accuracy ( 96.79% ) \n",
    "   - F1 Score ( weighted ) (Test Set):  0.9683\n",
    "   - Plot Importance Graph:\n",
    "     - It shows that I1 is the most important feature of all\n",
    "\n",
    "#### Till now I observed that removing correlated features in this dataset will not benefit the models to classify more accurate.\n",
    "\n",
    "#### 9. Now I have Visualized the Outliers inside the dataset :\n",
    "- Graphs :\n",
    " - BoxPlot\n",
    " - I Observed that X1 , X2 , X3 , X4 , X5 , X6 , I6 contains Outliers.\n",
    "\n",
    "#### 10. Remove the Outliers from the dataset.\n",
    "\n",
    "#### 11. Train the model on this dataset and check the model performance : \n",
    "\n",
    "- Logistic Regression :  \n",
    " - Accuracy :   Training Set ( 94% ) and Testing Set( 94% )\n",
    " - Precision(weighted) : Testing Set ( 0.95 )\n",
    " - Recall(weighted) : Testing Set ( 0.94 )\n",
    " - F1 Score(weighted) : Testing Set ( 0.94 )\n",
    "\n",
    "- SVM : \n",
    " - Accuracy : Training Set ( 94% ) and Testing Set( 94% )\n",
    " - Precision(weighted) : Testing Set( 0.95 )\n",
    " - Recall(weighted) : Testing Set( 0.94 )\n",
    " - F1 Score(weighted) : Testing Set( 0.95 )\n",
    "\n",
    "\n",
    "- XGBoost : \n",
    " - Accuracy : Training Set ( 97.10% ) and Testing Set( 96.87% )\n",
    " - F1 Score(weighted) : Testing Set( 0.9691 )\n",
    "\n",
    "#### I Observed that by removing outliers , model performance on testing dataset slightly increases.\n",
    "\n",
    "##### So, after going through all the procedures above , I conclude that removing of correlated features will not benefit us whereas\n",
    "##### removing of outliers from the dataset will slightly benefit us.\n",
    "\n",
    "#### 12. Now I have done all the preprocessing tasks. Now it's time to make efficient and powerful model for this task.\n",
    "- I have used one of the Ensembling Technique , known as Stacking \n",
    "- Description of My Stacked Model : \n",
    " - It Contains 2 Layers\n",
    "   - First Layer consists of 3 models : Logistic Regression , SVM , GradientBoostingClassifier.\n",
    "   - Second Layer consists of 1 model : XGBoost.\n",
    "- Accuracy : 97.14%\n",
    "- F1 Score : 0.9717   \n",
    "\n",
    "\n",
    "\n",
    "## Files : \n",
    "- ChallengeSolution.ipynb\n",
    " - It is the file where I have done all the procedures for this task with well documentation.\n",
    "- ChallengeSolution.html \n",
    " - It is the HTML file of ChallengeSolution.ipynb\n",
    "- Submit.csv\n",
    " - It is the file which contains prediction for land_test.csv\n",
    "\n",
    "# Note\n",
    "#### In Test File there are 20,00,000 rows which takes very much time even to load it.\n",
    "#### I will just take first 2,00,000 rows of data to make predictions. \n",
    "#### I can also use colab for this, But I start this task on my laptop so I can't shift to the colab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
